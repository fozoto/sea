# 正大战房实时数据生产环境部署文档

安装目录: /data/server

## 服务器公共配置

### 1. 配置host

将主机名指向内网ip, 而不是指向localhost

每台服务器都编辑`vim /etc/hosts`

```shell
192.168.31.121 e1031
192.168.31.95 e1032
192.168.31.147 e1033
192.168.33.199 e1019-0001
192.168.33.59 e1019-0002
192.168.33.107 e1019-0003
192.168.33.142 e1022

```

### 2. 挂载数据盘

`命令执行`root账号下:

```shell
## 创建硬盘
fdisk -l
fdisk /dev/vdb
输入n
输入p
默认直接回车
默认直接回车
默认直接回车
保存输入w

## 格式化成ext4格式
mkfs.ext4 /dev/vdb1

## 挂载到/data
mkdir /data
mount /dev/vdb1 /data

## 注册磁盘信息
cp /etc/fstab /etc/fstab_bak
echo `blkid /dev/vdb1 | awk '{print $2}'|sed 's/\"//g'` /data ext4 defaults 0 0 >> /etc/fstab

mkdir /data/server

## 查看/data大小
df -h
```

### 3. 复制安装文件到对应服务器

```shell
scp *gz root@192.168.31.121:/data/server
```

### 4. 创建hadoop用户

```shell
adduser hadoop
passwd hadoop
chown -R hadoop:hadoop /data
```

### 5. 配置环境变量

```shell
cp /etc/profile /etc/profile_bak
echo 'export JAVA_HOME=/data/server/jdk-8u191' >> /etc/profile
echo 'export SPARK_HOME=/data/server/spark-2.2.0-bin-hadoop2.7' >> /etc/profile
echo 'export HADOOP_HOME=/data/server/hadoop-2.7.3' >> /etc/profile
echo 'export KAFKA_HOME=/data/server/kafka_2.12-1.1.0' >> /etc/profile
echo 'export ZOOKEEPER_HOME=/data/server/zookeeper-3.4.6' >> /etc/profile
echo 'export FLUME_HOME=/data/server/flume-1.9.0' >> /etc/profile
echo 'export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$KAFKA_HOME/bin:$ZOOKEEPER_HOME/bin:$FLUME_HOME/bin' >> /etc/profile
```

### 6. 解压安装

```shell
su - hadoop

tar -zxvf hadoop-2.7.3.tar.gz
tar -zxvf spark-2.2.0-bin-hadoop2.7.tgz
source /etc/profile

## 验证
java -version
```

### 7. hadoop用户免密登录

```shell
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
ssh-copy-id hadoop@e1032
ssh-copy-id hadoop@e1033
```



## jdk

version: jdk-8u191-linux-x64

```shell
cd /data/server
tar -zxvf jdk-8u191-linux-x64.tar.gz
mv jdk1.8.0_191 jdk-8u191
```



## flume

version: apache-flume-1.9.0

从http接口获取的原始数据存放在/data/server/flume-1.9.0/files, 每次请求接口, 生成一个文件



### 1. 解压

```shell
cd /data/server/
tar -zxvf apache-flume-1.9.0-bin.tar.gz
mv apache-flume-1.9.0-bin flume-1.9.0
mkdir /data/server/flume-1.9.0/files
```

### 2. flume-env.sh

> mv /data/server/flume-1.9.0/conf/flume-env.sh.template vim /data/server/flume-1.9.0/conf/flume-env.sh  
>
> vim /data/server/flume-1.9.0/conf/flume-env.sh

```shell
export JAVA_HOME=/data/server/jdk-8u191
export JAVA_OPTS="$JAVA_OPTS -Xmx2048m"
```

### 3. flume-conf.properties

> mv /data/server/flume-1.9.0/conf/flume-conf.properties.template /data/server/flume-1.9.0/conf/flume-conf.properties  
>
> vim /data/server/flume-1.9.0/conf/flume-conf.properties

```shell
#agent
zdAgent.sources = zdHttpSrc
zdAgent.channels = zdKafkaChannel
zdAgent.sinks = zdKafkaSink

# sources
zdAgent.sources.zdHttpSrc.type = com.paic.pase.pd.zdflume.source.ZdHttpSource
zdAgent.sources.zdHttpSrc.heartbeat = 180 
zdAgent.sources.zdHttpSrc.filePath = /data/server/flume-1.9.0/files 
zdAgent.sources.zdHttpSrc.poolSize = 10
zdAgent.sources.zdHttpSrc.timeout = 600
zdAgent.sources.zdHttpSrc.channels = zdKafkaChannel

# channels
zdAgent.channels.zdKafkaChannel.type = org.apache.flume.channel.kafka.KafkaChannel
zdAgent.channels.zdKafkaChannel.kafka.bootstrap.servers = e1019-0001:9092,e1019-0002:9092,e1019-0003:9092
zdAgent.channels.zdKafkaChannel.kafka.topic = zd-flume-channel-topic
zdAgent.channels.zdKafkaChannel.kafka.consumer.group.id = zd-flume
zdAgent.channels.zdKafkaChannel.kafka.producer.max.request.size = 10240000

# sinks
zdAgent.sinks.zdKafkaSink.type = org.apache.flume.sink.kafka.KafkaSink
zdAgent.sinks.zdKafkaSink.kafka.bootstrap.servers = e1019-0001:9092,e1019-0002:9092,e1019-0003:9092
zdAgent.sinks.zdKafkaSink.kafka.producer.acks = 1
zdAgent.sinks.zdKafkaSink.kafka.producer.max.request.size = 10240000
zdAgent.sinks.zdKafkaSink.flumeBatchSize = 100
zdAgent.sinks.zdKafkaSink.channel = zdKafkaChannel
```

### 4. log4j.properties

> vim /data/server/flume-1.9.0/conf/log4j.properties

```shell
flume.root.logger=INFO,DAILY
flume.log.dir=./logs
flume.log.file=flume.log

log4j.logger.org.apache.flume.lifecycle = INFO
log4j.logger.org.jboss = WARN
log4j.logger.org.mortbay = INFO
log4j.logger.org.apache.avro.ipc.NettyTransceiver = WARN
log4j.logger.org.apache.hadoop = INFO
log4j.logger.org.apache.hadoop.hive = ERROR

# Define the root logger to the system property "flume.root.logger".
log4j.rootLogger=${flume.root.logger}


# Stock log4j rolling file appender
# Default log rotation configuration
log4j.appender.LOGFILE=org.apache.log4j.RollingFileAppender
log4j.appender.LOGFILE.MaxFileSize=100MB
log4j.appender.LOGFILE.MaxBackupIndex=10
log4j.appender.LOGFILE.File=${flume.log.dir}/${flume.log.file}
log4j.appender.LOGFILE.layout=org.apache.log4j.PatternLayout
log4j.appender.LOGFILE.layout.ConversionPattern=%d{dd MMM yyyy HH:mm:ss,SSS} %-5p [%t] (%C.%M:%L) %x - %m%n


# Warning: If you enable the following appender it will fill up your disk if you don't have a cleanup job!
# This uses the updated rolling file appender from log4j-extras that supports a reliable time-based rolling policy.
# See http://logging.apache.org/log4j/companions/extras/apidocs/org/apache/log4j/rolling/TimeBasedRollingPolicy.html
# Add "DAILY" to flume.root.logger above if you want to use this
log4j.appender.DAILY=org.apache.log4j.rolling.RollingFileAppender
log4j.appender.DAILY.rollingPolicy=org.apache.log4j.rolling.TimeBasedRollingPolicy
log4j.appender.DAILY.rollingPolicy.ActiveFileName=${flume.log.dir}/${flume.log.file}
log4j.appender.DAILY.rollingPolicy.FileNamePattern=${flume.log.dir}/${flume.log.file}.%d{yyyy-MM-dd}
log4j.appender.DAILY.layout=org.apache.log4j.PatternLayout
log4j.appender.DAILY.layout.ConversionPattern=%d{dd MMM yyyy HH:mm:ss,SSS} %-5p [%t] (%C.%M:%L) %x - %m%n


# console
# Add "console" to flume.root.logger above if you want to use this
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d (%t) [%p - %l] %m%n

```

### 5. run.sh

> vim /data/server/flume-1.9.0/run.sh

```shell
#!/bin/bash

FLUME_HOME="/data/server/flume-1.9.0"
FLUME_CONF_FILE="flume-conf.properties"
FLUME_NAME="flume"
FLUME_AGENT="zdAgent"

function start() {
    if [ ! -f "$FLUME_HOME/pid" ]; then
        nohup flume-ng agent --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf/flume-conf.properties --name zdAgent > /dev/null 2>&1 & echo $! > $FLUME_HOME/pid
        echo "start success..."
        echo "log: $FLUME_HOME/logs/flume.log"
    else
        echo "flume process exists, start failed!"
    fi
}

function stop() {
    if [ ! -f "$FLUME_HOME/pid" ]; then
        echo "flume process stoped."
    else
        kill -9 `cat $FLUME_HOME/pid`
        rm $FLUME_HOME/pid
        echo "flume process stop success!"
    fi
}

function restart() {
    stop
    sleep 3
    start
}

case "$1" in
    "start")
        start
        ;;
     "stop")
        stop
    ;;
     "restart")
        restart
        ;;
     *)
        echo "please enter start, stop, restart"
esac
```

> chmod +x /data/server/run.sh

### 6. 上传自定义source到lib

自定义的flume source依赖fastjson

```shell
scp /tmp/fastjson-1.2.62.jar hadoop@192.168.33.142:/data/server/flume-1.9.0/lib
scp /tmp/zd-flume-source-1.0-RELEASE.jar hadoop@192.168.33.142:/data/server/flume-1.9.0/lib
```

### 7. 启动与停止

```shell
## 启动
/data/server/flume-1.9.0/run.sh start
## 停止
/data/server/flume-1.9.0/run.sh stop
```



## kafka

version: kafka_2.12-1.1.0

```shell
tar -zxvf /data/server/kafka_2.12-1.1.0.tgz
```

### 1. server.properties

```shell
message.max.bytes=10240000
zookeeper.connect=e1019-0001:2181,e1019-0001:2181,e1019-0003:2181
```

### 2. producer.properties

```shell
bootstrap.servers=e1019-0001:9092,e1019-0002:9092,e1019-0003:9092
max.request.size=10240000
```

### 3. 启动

```shell
kafka-server-start.sh -daemon /data/server/kafka_2.12-1.1.0/config/server.properties
```



## zookeeper

version: zookeeper-3.4.6

### 1. 解压

```shell
tar -zxvf /data/server/zookeeper-3.4.6.tar.gz
mkdir /data/zookeeper
cp /data/server/zookeeper-3.4.6/conf/zoo_sample.cfg /data/server/zookeeper-3.4.6/conf/zoo.cfg
```

### 2. 配置zoo.cfg

```shell
dataDir=/data/zookeeper
server.1=e1019-0001:2888:3888
server.2=e1019-0002:2888:3888
server.3=e1019-0003:2888:3888
```

 ### 3. 配置myid

```shell
## 在e1019-0001服务器
echo '1' > /data/zookeeper/myid

## 在e1019-0002服务器
echo '2' > /data/zookeeper/myid

## 在e1019-0003服务器
echo '3' > /data/zookeeper/myid
```

### 4. 启动

```shell
## 在e1019-0001服务器
zkServer.sh start

## 在e1019-0002服务器
zkServer.sh start

## 在e1019-0003服务器
zkServer.sh start

```



## Hadoop

version: hadoop-2.7.3

### 1. 创建目录

```shell
mkdir -p /data/dfs/tmp
mkdir -p /data/dfs/name
mkdir -p /data/dfs/data
```

### 2. hadoop-env.sh

> vim /data/server/hadoop-2.7.3/etc/hadoop/hadoop-env.sh

```shell
export JAVA_HOME=/data/server/jdk-8u191
export HDFS_NAMENODE_USER="hadoop"
export HDFS_DATANODE_USER="hadoop"
export HDFS_SECONDARYNAMENODE_USER="hadoop"
```

### 3.  core-site.xml

> vim /data/server/hadoop-2.7.3/etc/hadoop/hdfs-site.xml

```xml
<configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://e1031:9000</value>
         </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>file:/data/dfs/tmp</value>
        </property>
        <property>
                <name>hadoop.proxyuser.hadoop.hosts</name>
                <value>*</value>
        </property>
        <property>
                <name>hadoop.proxyuser.hadoop.groups</name>
                <value>*</value>
        </property>
        <property>
                <name>hadoop.http.staticuser.user</name>
                <value>hadoop</value>
        </property>
</configuration>
```

### 4. hdfs-site.xml

> vim /data/server/hadoop-2.7.3/etc/hadoop/core-site.xml

```xml
<configuration>
        <property>
                <name>dfs.replication</name>
                <value>3</value>
        </property>
        <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>e1031:9868</value>
        </property>
                <property>
                <name>dfs.name.dir</name>
                <value>/data/dfs/name</value>
        </property>

         <property>
                <name>dfs.data.dir</name>
                <value>/data/dfs/data</value>
        </property>
        <property>
                <name>dfs.webhdfs.enabled</name>
                <value>true</value>
        </property>

</configuration>
```

### 5.  mapred-site.xml

> mv /data/server/hadoop-2.7.3/etc/hadoop/mapred-site.xml.template  /data/server/hadoop-2.7.3/etc/hadoop/mapred-site.xml  
>
> vim /data/server/hadoop-2.7.3/etc/hadoop/mapred-site.xml

```xml
<configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>yarn.app.mapreduce.am.env</name>
                <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
                <name>mapreduce.map.env</name>
                <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
                <name>mapreduce.reduce.env</name>
                <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
                <name>mapreduce.map.memory.mb</name>
                <value>2048</value>
        </property>
        <property>
            <name>mapreduce.jobhistory.address</name>
            <value>e1031:10020</value>         
        </property>
        <property>
            <name>mapreduce.jobhistory.webapp.address</name>
            <value>e1031:19888</value>
        </property>

</configuration>
```

### 6. yarn-site.xml

> vim /data/server/hadoop-2.7.3/etc/hadoop/yarn-site.xml

```xml
<configuration>

<!-- Site specific YARN configuration properties -->
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>e1031</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>

        <property>
                <name>yarn.nodemanager.vmem-check-enabled</name>
                <value>false</value>
        </property>
        <property>
                <name>yarn.nodemanager.vmem-pmem-ratio</name>
                <value>5</value>
        </property>

        <property>
          <name>yarn.nodemanager.localizer.cache.cleanup.interval-ms</name>
          <value>600000</value>
    </property>
    <property>
          <name>yarn.log-aggregation.retain-seconds</name>
          <value>604800</value>
      </property>
      <property>
                 <name>yarn.log.server.url</name>
                <value>http://e1031:19888/jobhistory/logs/</value>
        </property>

        <property>
                  <name>yarn.log-aggregation-enable</name>
                  <value>true</value>
        </property>
</configuration>
```

### 7. 格式化namenode

```shell
## 格式化namenode
hdfs namenode -format
```

### 8. 启动与停止

```shell
## 启动
start-dfs.sh
## 停止
stop-dfs
```



## spark

version: spark-2.2.0-bin-hadoop2.7

### 1. spark-env.sh

> vim /data/server/spark-2.2.0-bin-hadoop2.7/conf/spark-env.sh

``` shell
export JAVA_HOME=/data/server/jdk-8u191
export HADOOP_HOME=/data/server/hadoop-2.7.3
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_MASTER_IP=e1031
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://192.168.31.121:9000/history"
export SPARK_DIST_CLASSPATH=$(/data/server/hadoop-2.7.3/bin/hadoop classpath)
hdfs dfs -mkdir hdfs://e1031:9000/history
```

### 2. slaves

> cp /data/server/spark-2.2.0-bin-hadoop2.7/conf/slaves.template /data/server/spark-2.2.0-bin-hadoop2.7/conf/slaves  
>
> vim /data/server/spark-2.2.0-bin-hadoop2.7/conf/slaves

```shell
e1031
e1032
e1033
```

### 3. spark-defaults.conf

> cp /data/server/spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf.template /data/server/spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf  
>
> vim /data/server/spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf

```shell
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://e1031:9000/history
spark.eventLog.compress          true
spark.yarn.jars         hdfs://e1031:9000/spark-runtime/jars/*
```

### 4. 复制hadoop的配置文件到spark

```shell
cp /data/server/hadoop-2.7.3/etc/hadoop/*.xml /data/server/spark-2.2.0-bin-hadoop2.7/conf
```

### 5. 启动与停止

```shell
## 启动
start-master.sh
start-slaves.sh
## 停止
stop-master.sh
stop-slaves.sh
```

 

## zd-real-data

这是我们开发的jar包

### 1. 上传文件

```shell
## 在e1031执行
mkdir -p /data/server/zd-real-data/lib
mkdir -p /data/server/zd-real-data/logs
mkdir -p /data/server/zd-real-data/conf
## 其他服务器上传jar到e1031
scp zd-real-data-0.1.jar hadoop@192.168.31.121:/data/server/zd-real-data/lib
scp ojdbc7.jar hadoop@192.168.31.121:/data/server/zd-real-data/lib
```

### 2.  application-egg.conf

> vim /data/server/zd-real-data/conf/application-egg.conf

```properties
task {
  app-name: "egg"
  duration = 10000
  conf {
    spark.sql.shuffle.partitions = 4
    spark.cleaner.referenceTracking.cleanCheckpoints = true
    //spark.sql.autoBroadcastJoinThreshold = -1
    spark.streaming.concurrentJobs = 1
  }

  checkpoint-location = "/checkpoint/zd-egg/"
  task : com.paic.pase.pd.zdrealdata.tasks.EggTask
}

mode = true

kafka {
  group-id = "zd-spark-egg-01"
  bootstrap = "e1019-0001:9092,e1019-0002:9092,e1019-0003:9092"
  topics = ["zd.flume.egg","zd.flume.eggFarmDt"]
  params = {
    "enable.auto.commit" : false
    "auto.offset.reset" : "earliest"
    "session.timeout.ms" : 60000
    "request.timeout.ms" : 90000
  }
}

egg{
  egg-real : ["zd.flume.egg"]
  egg-farm : ["zd.flume.eggFarmDt"]
}


jdbc {
  driver: "oracle.jdbc.OracleDriver"
  url: "jdbc:oracle:thin:@(description=(address_list= (address=(host=192.168.31.xxx) (protocol=tcp)(port=1521))(address=(host=192.168.31.139)(protocol=tcp) (port=1521)) (load_balance=yes)(failover=yes))(connect_data=(service_name= rac)))"
  user: "xxxxx"
  password: "xxxx"
}

output{
  jdbc {
    driver: "oracle.jdbc.OracleDriver"
    url: "jdbc:oracle:thin:@(description=(address_list= (address=(host=192.168.31.xxx) (protocol=tcp)(port=1521))(address=(host=192.168.31.139)(protocol=tcp) (port=1521)) (load_balance=yes)(failover=yes))(connect_data=(service_name= rac)))"
    user: "xxxxx"
    password: "xxxxx"
  }
}
```

### 3. application-food.conf

> vim /data/server/zd-real-data/conf/application-food.conf

```properties
task {
  app-name: "food"
  duration = 10000
  conf {
//    spark config
//    spark.master : "local[4]"
    spark.sql.shuffle.partitions = 4
    spark.cleaner.referenceTracking.cleanCheckpoints = true
    //spark.sql.autoBroadcastJoinThreshold = -1
    spark.streaming.concurrentJobs = 1
  }

  checkpoint-location = "/checkpoint/zd-food/"
  task : com.paic.pase.pd.zdrealdata.tasks.FoodTask
}

mode = true

kafka {
  group-id = "zd-spark-food-01"
  bootstrap = "e1019-0001:9092,e1019-0002:9092,e1019-0003:9092"
  topics = ["zd.flume.food"]
  params = {
    "enable.auto.commit" : false
    "auto.offset.reset" : "earliest"
    "session.timeout.ms" : 60000
    "request.timeout.ms" : 90000
    //    "" : ""
  }
}

egg{
  egg-real : ["zd.flume.egg"]
  egg-farm : ["zd.flume.eggFarmDt"]
}


jdbc {
  driver: "oracle.jdbc.OracleDriver"
  url: "jdbc:oracle:thin:@(description=(address_list= (address=(host=192.168.31.xxx) (protocol=tcp)(port=1521))(address=(host=192.168.31.xxx)(protocol=tcp) (port=1521)) (load_balance=yes)(failover=yes))(connect_data=(service_name= rac)))"
  user: "xxxxx"
  password: "xxxxx"
}

output{
  jdbc {
    driver: "oracle.jdbc.OracleDriver"
    url: "jdbc:oracle:thin:@(description=(address_list= (address=(host=192.168.31.xxx) (protocol=tcp)(port=1521))(address=(host=192.168.31.xxx)(protocol=tcp) (port=1521)) (load_balance=yes)(failover=yes))(connect_data=(service_name= rac)))"
    user: "xxxxx"
    password: "xxxxx"
  }
}
```

### 4. log4j.properties

```properties
# Set everything to be logged to the console
log4j.rootCategory=INFO, console, DAILY
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

log4j.appender.DAILY=org.apache.log4j.rolling.RollingFileAppender
log4j.appender.DAILY.rollingPolicy=org.apache.log4j.rolling.TimeBasedRollingPolicy
log4j.appender.DAILY.rollingPolicy.ActiveFileName=${LOG_FILE}
log4j.appender.DAILY.rollingPolicy.FileNamePattern=${LOG_FILE}.%d{yyyy-MM-dd}
log4j.appender.DAILY.layout=org.apache.log4j.PatternLayout
log4j.appender.DAILY.layout.ConversionPattern=%d{dd MMM yyyy HH:mm:ss,SSS} %-5p [%t] (%C.%M:%L) %x - %m%n

# Set the default spark-shell log level to WARN. When running the spark-shell, the
# log level for this class is used to overwrite the root logger's log level, so that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=WARN

# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark_project.jetty=WARN
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR

# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
```

### 5. run.sh

> vim /data/server/zd-real-data/run.sh

```shell
#!/bin/sh

ROOT="/data/server/zd-real-data"

## 现代食品参数设置
function food() {
    NAME="food"
    CONF="$ROOT/conf/application-food.conf"
    PID="$ROOT/pid-food"
    CLASS="com.paic.pase.pd.zdrealdata.SparkStreamsApp"
    LOGGING_OPTS="-Dlog4j.configuration=file:$ROOT/conf/log4j.properties -DLOG_FILE=$ROOT/logs/$NAME/$NAME.log"
}

## 鸡蛋参数设置
function egg() {
    NAME="egg"
    CONF="$ROOT/conf/application-egg.conf"
    PID="$ROOT/pid-egg"
    CLASS="com.paic.pase.pd.zdrealdata.SparkStreamsApp"
    LOGGING_OPTS="-Dlog4j.configuration=file:$ROOT/conf/log4j.properties -DLOG_FILE=$ROOT/logs/$NAME/$NAME.log"
}

## 各个模块都调用的启动方法
function start() {
    if [ ! -d "$ROOT/logs/$NAME" ]; then
        mkdir -p $ROOT/logs/$NAME
    fi

    if [ ! -f "$PID" ]; then
        nohup spark-submit --name=$NAME --conf="spark.driver.extraJavaOptions=$LOGGING_OPTS" --driver-memory 2048M --executor-memory 20G --total-executor-cores 16 --master spark://e1031:7077 --supervise --class $CLASS --jars $ROOT/lib/ojdbc7.jar $ROOT/lib/zd-real-data-0.1.jar $CONF > /dev/null 2>&1 & echo $! > $PID
        echo "start $NAME success!"
    else
        echo "$NAME already running, start failed!!!!!!!!!!!!!!!!!!!!"
    fi
}

## 各个模块都调用的停止方法
function stop() {
    if [ ! -f "$PID" ]; then
        echo "$NAME stoped."
    else
        kill -9 `cat $PID`
        hadoop fs -rm -r `cat $CONF | grep checkpoint-location | awk -F'=' '{print $2}' | sed 's/\"//g'`
        rm $PID
        echo "$NAME stop success!"
    fi
}

## 启动现代食品
function start_food() {
    food
    start
}

## 停止现代食品
function stop_food() {
    food
    stop
}

## 启动鸡蛋
function start_egg() {
    egg
    start
}

## 停止鸡蛋
function stop_egg() {
    egg
    stop
}

case "$1" in
    "food-start")
        start_food
    ;;
    "food-stop")
        stop_food
    ;;
    "egg-start")
        start_egg
    ;;
    "egg-stop")
        stop_egg
    ;;
    *)
        echo "please enter food-start, food-stop, egg-start, egg-stop"
esac
```

> chmod +x /data/server/zd-real-data/run.sh

## kafka eagle安装

**目前安装在d1005**

1. 通过 http://download.kafka-eagle.org/ 下载最新的kafka-eagle
2. 解压到`/data/server/kafka-eagle`
3. `~/.bashrc`配置环境变量 `export KE_HOME=/data/server/kafka-eagle`
4. 修改配置文件 `$KE_HOME\conf\system-config.properties` 中kafka连接信息
    ```conf
    ######################################
    # multi zookeeper&kafka cluster list
    ######################################
    kafka.eagle.zk.cluster.alias=cluster1
    cluster1.zk.list=e1019-0001:2181,e1019-0002:2181,e1019-0003:2181
    ```
5. 添加kafka的`hosts`信息
    ```hosts
    192.168.30.219	d1005	d1005
    
    192.168.33.142 e1022
    192.168.31.121 e1031
    192.168.31.95 e1032
    192.168.31.147 e1033
    192.168.33.199 e1019-0001
    192.168.33.59 e1019-0002
    192.168.33.107 e1019-0003
    ```
6. 启动`bin/ke.sh start`

## 常用命令

### kafka

```shell
## 查看topic
kafka-topics.sh --zookeeper=e1019-0001:2181,e1019-0002:2181,e1019-0003:2181 --list

## 查看topic数据
kafka-console-consumer.sh --bootstrap-server=e1019-0001:9092,e1019-0002:9092,e1019-0003:9092 --topic=zd.flume.eggFarmDt --from-beginning
```

